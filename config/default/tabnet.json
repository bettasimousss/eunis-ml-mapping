{
        
    'hyperparams': {
         ### Architecture
        'n_d': 16, ## width of decision layer, range between 8 and 64
        'n_a': 16, ## choose equal to n_d
        'n_steps': 3, ## number of steps, range between 3 and 10
         ### Feature selection and regularization
        'mask_type' : 'sparsemax',
        'gamma': 1.3, ## feature reusage, range between 1 (least correlated) and 2 (most correlated)
        'lambda_sparse': 1e-3, ## Sparsity in feature selection,
        ## Other
        'verbose': 1,
        'device_name': 'cpu',
        'scheduler_params': {'step_size':10, 'gamma':0.9}
    },

    'train_params' : {
        'optim': torch.optim.Adam,
        'learning_rate': 0.01,
        'attention_level' : 'feature',
        'loss' : {'loss_fn':'ldam', 'margin':0.5},
        'pretraining': False,      
        'eval_metric': ['balanced_accuracy','accuracy'],
        'max_epochs': 100,
        'weights':1,  ## use 1 for inverse class occurrence sampling
        'num_workers': 10
    }
}